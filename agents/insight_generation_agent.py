import pandas as pd
import os
from collections import Counter
import re
import logging
import traceback
from typing import List, Dict, Optional

from .groq_client import create_groq_client, call_groq_api
from rate_limit_handler import apply_rate_limit_handling

@apply_rate_limit_handling
class InsightGenerationAgent:
    """
    Agent 2: Insight Generation & Automation Identification
    - Analyzes ticket data to detect patterns
    - Identifies potential automation opportunities
    - Cross-checks data against keywords if provided
    """
    
    def __init__(
        self, 
        pattern_threshold: float = 0.05,  # 5% of tickets
        reassignment_threshold: float = 0.1,  # 10% of tickets
        long_ticket_threshold: int = 24  # 24 hours
    ):
        # Set up logging
        logging.basicConfig(
            level=logging.INFO, 
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

        # Configurable thresholds
        self.pattern_threshold = pattern_threshold
        self.reassignment_threshold = reassignment_threshold
        self.long_ticket_threshold = long_ticket_threshold

        # Initialize GROQ client
        self.client, self.model_name, self.error_message = create_groq_client()
        if self.error_message:
            self.logger.warning(f"GROQ Client Initialization Warning: {self.error_message}")
        
        # Define automation patterns to look for
        self.automation_patterns = {
            'password_reset': [
                'password reset', 'forgot password', 'reset password', 
                'password expired', 'change password'
            ],
            'access_request': [
                'access request', 'permission', 'grant access', 'request access',
                'access to', 'authorize', 'authorization', 'privilege'
            ],
            'data_entry': [
                'data entry', 'input data', 'enter data', 'form', 'manual entry',
                'spreadsheet', 'excel', 'copy paste', 'copy and paste'
            ],
            'report_generation': [
                'report', 'generate report', 'reporting', 'dashboard', 'monthly report',
                'weekly report', 'data export', 'export'
            ],
            'account_management': [
                'create account', 'new account', 'account creation', 'onboarding',
                'offboarding', 'disable account', 'remove account', 'delete user'
            ],
            'system_alerts': [
                'alert', 'monitoring', 'notification', 'disk space', 'memory',
                'cpu', 'server down', 'outage', 'system down'
            ],
            'routine_maintenance': [
                'maintenance', 'update', 'patch', 'backup', 'cleanup',
                'regular', 'scheduled', 'recurring', 'routine'
            ]
        }
    

    def _check_automation_patterns(self, data):
        """Check for predefined automation patterns in the data"""
        results = {category: 0 for category in self.automation_patterns.keys()}
        
        # Check in description and resolution fields
        for idx, row in data.iterrows():
            description = str(row.get('description', ''))
            resolution = str(row.get('resolution', ''))
            
            for category, patterns in self.automation_patterns.items():
                if any(pattern in description.lower() for pattern in patterns) or \
                any(pattern in resolution.lower() for pattern in patterns):
                    results[category] += 1
        
        return results

    def _get_llm_insights(self, data):
        """
        Use GROQ LLM to get additional insights
        
        Args:
            data (pd.DataFrame): Processed ticket data
            
        Returns:
            list: List of insights generated by the LLM
        """
        if not self.client:
            print("GROQ client not available, skipping LLM insights")
            return []
        
        try:
            # Prepare a sample of the data for analysis
            sample_size = min(100, len(data))
            sample_data = data.sample(sample_size) if len(data) > sample_size else data
            
            # Convert sample data to a string representation
            data_str = sample_data.head(20).to_string()
            
            # Prepare prompt for the LLM
            prompt = f"""
            You are an IT automation consultant analyzing support ticket data to identify automation opportunities.
            
            Based on the sample ticket data, identify 2-3 clear opportunities for automation.
            For each opportunity:
            1. Provide a concise title
            2. Describe the issue pattern you've identified
            3. Explain why it's suitable for automation
            4. Rate the potential impact as high, medium, or low
            
            Sample ticket data:
            {data_str}
            
            Format your response as follows for each opportunity:
            TITLE: [Opportunity Title]
            PATTERN: [Identified Pattern]
            AUTOMATION_POTENTIAL: [Why It Can Be Automated]
            IMPACT: [high/medium/low]
            """
            
            # Call the GROQ API
            response_text, error = call_groq_api(
                self.client, 
                self.model_name,
                prompt,
                max_tokens=1000,
                temperature=0.2
            )
            
            if error:
                print(f"Error getting LLM insights: {error}")
                return []
                
            # Parse the response
            return self._parse_llm_insights(response_text)
        
        except Exception as e:
            print(f"Error getting LLM insights: {str(e)}")
            return []

    def _parse_llm_insights(self, insights_text):
        """
        Parse the LLM response to extract structured automation opportunities
        
        Args:
            insights_text (str): The raw text response from the LLM
            
        Returns:
            list: List of structured automation opportunities
        """
        opportunities = []
        
        # Split the text into sections for each opportunity
        sections = re.split(r'TITLE:', insights_text)[1:]  # Skip the first empty part
        
        for section in sections:
            try:
                # Extract fields using regex
                title_match = re.search(r'(.+?)(?=PATTERN:|$)', section)
                pattern_match = re.search(r'PATTERN:(.+?)(?=AUTOMATION_POTENTIAL:|$)', section)
                potential_match = re.search(r'AUTOMATION_POTENTIAL:(.+?)(?=IMPACT:|$)', section)
                impact_match = re.search(r'IMPACT:(.+?)(?=$|\n)', section)
                
                if title_match:
                    title = title_match.group(1).strip()
                    pattern = pattern_match.group(1).strip() if pattern_match else ""
                    automation_potential = potential_match.group(1).strip() if potential_match else ""
                    impact = impact_match.group(1).strip().lower() if impact_match else "medium"
                    
                    # Ensure impact is one of the expected values
                    if impact not in ['high', 'medium', 'low']:
                        impact = "medium"
                    
                    # Create the opportunity object
                    opportunity = {
                        'title': title,
                        'description': f"{pattern}\n\nAutomation potential: {automation_potential}",
                        'category': 'llm_identified',
                        'impact': impact
                    }
                    
                    opportunities.append(opportunity)
            except Exception as e:
                print(f"Error parsing LLM insight section: {str(e)}")
                continue
        
        return opportunities


    def generate_insights(
        self, 
        data: pd.DataFrame, 
        keywords: Optional[List[str]] = None
    ) -> List[Dict[str, str]]:
        """
        Generate insights and identify automation opportunities
        
        Args:
            data (pd.DataFrame): Processed ticket data
            keywords (list, optional): List of keywords to cross-check
            
        Returns:
            list: List of automation opportunities with title and description
        """
        # Get basic statistics
        total_tickets = len(data)
        
        # Identify common patterns
        automation_opportunities = []
        
        # 1. Check for predefined automation patterns
        pattern_results = self._check_automation_patterns(data)
        for category, count in pattern_results.items():
            if count > 0 and (count / total_tickets) > self.pattern_threshold:
                opportunity = self._create_opportunity_from_pattern(category, count, total_tickets, data)
                automation_opportunities.append(opportunity)
        
        # 2. Look for reassignment patterns
        if 'reassignment_indicator' in data.columns:
            reassigned_tickets = data[data['reassignment_indicator'] == True]
            if len(reassigned_tickets) > 0 and (len(reassigned_tickets) / total_tickets) > self.reassignment_threshold:
                opportunity = {
                    'title': 'Reduce Ticket Reassignments with Intelligent Routing',
                    'description': f'Found {len(reassigned_tickets)} tickets ({(len(reassigned_tickets)/total_tickets)*100:.1f}%) that were reassigned. Implementing intelligent ticket routing based on keywords and patterns could reduce handling time and improve first-contact resolution rates.',
                    'category': 'workflow',
                    'impact': 'medium',
                    'sample_tickets': reassigned_tickets['ticket_id'].head(5).tolist() if 'ticket_id' in reassigned_tickets.columns else []
                }
                automation_opportunities.append(opportunity)
        
        # 3. Look for long-duration tickets
        if 'duration_hours' in data.columns:
            long_tickets = data[data['duration_hours'] > self.long_ticket_threshold]
            if len(long_tickets) > 0 and (len(long_tickets) / total_tickets) > 0.2:
                opportunity = {
                    'title': 'Accelerate Resolution for Time-Consuming Tickets',
                    'description': f'Identified {len(long_tickets)} tickets ({(len(long_tickets)/total_tickets)*100:.1f}%) taking more than {self.long_ticket_threshold} hours to resolve. Implementing self-service solutions or automated workflows for these issues could significantly reduce resolution times.',
                    'category': 'efficiency',
                    'impact': 'high',
                    'sample_tickets': long_tickets['ticket_id'].head(5).tolist() if 'ticket_id' in long_tickets.columns else []
                }
                automation_opportunities.append(opportunity)
        
        # 4. Cross-check with provided keywords if available
        if keywords and len(keywords) > 0:
            keyword_opportunities = self._analyze_with_keywords(data, keywords)
            automation_opportunities.extend(keyword_opportunities)
        
        # 5. Use GROQ to get additional insights if available
        if self.client:
            try:
                ai_opportunities = self._get_llm_insights(data)
                automation_opportunities.extend(ai_opportunities)
            except Exception as e:
                self.logger.error(f"Error getting LLM insights: {str(e)}")
                traceback.print_exc()
        
        # Sort opportunities by potential impact
        automation_opportunities = sorted(
            automation_opportunities, 
            key=lambda x: 0 if 'impact' not in x else (
                0 if x['impact'] == 'high' else (
                    1 if x['impact'] == 'medium' else 2
                )
            )
        )
        
        return automation_opportunities

    # ... (rest of the methods remain the same as in the original implementation)

    def _analyze_with_keywords(self, data: pd.DataFrame, keywords: List[str]) -> List[Dict[str, str]]:
        """
        Improve keyword analysis with more robust matching
        """
        def fuzzy_keyword_match(text: str, keywords: List[str]) -> List[str]:
            """
            Perform fuzzy matching of keywords in text
            """
            matched_keywords = []
            for keyword in keywords:
                # Case-insensitive partial match
                if keyword.lower() in text.lower():
                    matched_keywords.append(keyword)
            return matched_keywords

        opportunities = []
        
        # Create a word frequency counter
        word_freq = Counter()
        
        # Check for keywords in description and resolution
        keyword_matches = {keyword: 0 for keyword in keywords}
        
        for idx, row in data.iterrows():
            description = str(row.get('description', ''))
            resolution = str(row.get('resolution', ''))
            combined_text = (description + ' ' + resolution).lower()
            
            # Use fuzzy matching
            for keyword in keywords:
                if keyword.lower() in combined_text:
                    keyword_matches[keyword] += 1
        
        # Find keywords with significant occurrences
        significant_keywords = {
            k: v for k, v in keyword_matches.items() 
            if v > len(data) * self.pattern_threshold
        }
        
        if significant_keywords:
            # Group similar keywords
            grouped_keywords = {}
            for kw, count in significant_keywords.items():
                # Simple grouping based on first word
                key_term = kw.split()[0] if ' ' in kw else kw
                if key_term not in grouped_keywords:
                    grouped_keywords[key_term] = []
                grouped_keywords[key_term].append((kw, count))
            
            # Create opportunities for each group
            for key_term, keyword_counts in grouped_keywords.items():
                total_count = sum(count for _, count in keyword_counts)
                keywords_list = ", ".join([kw for kw, _ in keyword_counts])
                
                opportunity = {
                    'title': f'Automation Opportunity for "{key_term.title()}" Related Issues',
                    'description': f'Found {total_count} tickets ({(total_count/len(data))*100:.1f}%) containing keywords: {keywords_list}. These could indicate a repetitive process suitable for automation.',
                    'category': 'keyword_identified',
                    'impact': 'medium' if total_count > len(data) * 0.1 else 'low',
                    'keywords': [kw for kw, _ in keyword_counts]
                }
                opportunities.append(opportunity)
        
        return opportunities